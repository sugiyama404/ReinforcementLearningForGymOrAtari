{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2CForCarRacing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMYM5hLNXh9ab258ftfNOEs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinforcementLearningForGymOrAtari/blob/main/A2C/A2CForCarRacing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8bCDqyTz9Wp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7025c01e-f4b7-4b88-ec72-f2fa23073397"
      },
      "source": [
        "!pip uninstall gym -y # gym 0.17.3 was broken at 2021/11/08\n",
        "!pip install gym gym[box2d] tensorflow-addons  > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.17.3\n",
            "Uninstalling gym-0.17.3:\n",
            "  Successfully uninstalled gym-0.17.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cck23Zda0D63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44eef870-97ee-444d-aa34-9e298e31cecc"
      },
      "source": [
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, ReLU, Input, Lambda, Conv2D, Flatten\n",
        "from tensorflow.keras.losses import Huber\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.optimizers import RectifiedAdam\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "import math\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "!apt update  > /dev/null 2>&1\n",
        "!apt install xvfb  > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay  > /dev/null 2>&1\n",
        "from pyvirtualdisplay import Display\n",
        "d = Display()\n",
        "d.start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f54dc6cb810>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs85f7vT_Ze8"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        obs_shape = (96,96,3)\n",
        "        nb_actions = 5\n",
        "        opt = RectifiedAdam(learning_rate=0.0001, epsilon=0.001)\n",
        "        input_ = inputs = Input(shape=obs_shape)\n",
        "        loss=Huber()\n",
        "        common = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), activation='relu')(inputs)\n",
        "        common = Conv2D(64, kernel_size=(4, 4), strides=(4, 4), activation='relu')(common)\n",
        "        common = Conv2D(64, kernel_size=(3, 3), strides=(3, 3), activation='relu')(common)\n",
        "        common = Flatten()(common)\n",
        "\n",
        "        common = Dense(512, activation='relu')(common)\n",
        "        actor_layer  = Dense(nb_actions, activation=\"softmax\")(common)\n",
        "        critic_layer = Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = Model(input_, [actor_layer, critic_layer])\n",
        "        model.compile(loss = loss, optimizer=opt)\n",
        "        model.summary()\n",
        "        Brain.model = model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lem_RNKr_4ch"
      },
      "source": [
        "class Actor(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        ps, _ = self.model(np.array([state]))\n",
        "        act_p = ps[0].numpy()\n",
        "        act_p = act_p if (not (np.isnan(act_p).any())) else self._nan_to_zero_softmax(act_p)[0]\n",
        "        return np.random.choice(5, p=act_p)\n",
        "\n",
        "    def _nan_to_zero_softmax(self, x):\n",
        "        x[np.isnan(x)] = 0\n",
        "        return self._softmax(x)\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        if (x.ndim == 1):\n",
        "            x = x[None,:]\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dkKYPhDAFwZ"
      },
      "source": [
        "class Critic(Brain):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.gamma = 0.99\n",
        "        self.beta  = 0.1\n",
        "\n",
        "    def valuenetwork(self, val):\n",
        "        states, next_states, actions = val['state'], val['next_state'], val['act']\n",
        "        rewards, dones = val['reward'], val['done']\n",
        "\n",
        "        onehot_actions = tf.one_hot(actions, 5)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = Brain.model(states, training=True)\n",
        "            _, next_v = Brain.model(next_states, training=True)\n",
        "\n",
        "            a_pi = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            a_pi = tf.clip_by_value(a_pi, 1e-10, 1.0)\n",
        "\n",
        "            q = rewards + (1 - dones) * self.gamma * next_v\n",
        "            advantage = q - v\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage, a_pi, v)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, Brain.model.trainable_variables)\n",
        "        Brain.model.optimizer.apply_gradients(zip(gradients, Brain.model.trainable_variables))\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,a_pi,v):\n",
        "\n",
        "        a = tf.math.log(a_pi) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "        sigma = tf.math.reduce_std(v)\n",
        "        sigma = tf.math.square(sigma)\n",
        "        entropy = self.beta*0.5*(tf.math.log(2 * math.pi * sigma) + 1)\n",
        "        return entropy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m781NcBNAcHG"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : np.ndarray = np.empty((0,96, 96, 3))\n",
        "    next_state : np.ndarray = np.empty((0,96, 96, 3))\n",
        "    action : np.ndarray = np.array([],int)\n",
        "    reward : np.ndarray = np.array([])\n",
        "    done : np.ndarray = np.array([])\n",
        "    batch_size : int = 32\n",
        "\n",
        "    def reset_experiences(self):\n",
        "        self.state = np.empty((0,96, 96, 3))\n",
        "        self.next_state = np.empty((0,96, 96, 3))\n",
        "        self.action = np.array([],int)\n",
        "        self.reward = np.array([])\n",
        "        self.done = np.array([])\n",
        "\n",
        "    def set_experiences(self, state, next_state, action, reward, done):\n",
        "        state = np.reshape(state, [1, 96, 96, 3])\n",
        "        self.state = np.append(self.state, state, axis=0)\n",
        "        next_state = np.reshape(next_state, [1, 96, 96, 3])\n",
        "        self.next_state = np.append(self.next_state, next_state, axis=0)\n",
        "        self.action = np.append(self.action, np.array(action))\n",
        "        self.reward = np.append(self.reward, np.array(reward))\n",
        "        self.done = np.append(self.done, np.array(done))\n",
        "\n",
        "    def get_experiences(self):\n",
        "        mb_index = np.random.choice(len(self.action), self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.state[mb_index], self.next_state[mb_index],\n",
        "                 self.action[mb_index], self.reward[mb_index], self.done[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1\n",
        "\n",
        "    def isGetter(self):\n",
        "        return True if (len(self.action) > self.batch_size) else False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9N6dj4MA5Jy"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, experiences, episodes_times = 1000):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.experiences = experiences\n",
        "        self.episodes_times = episodes_times\n",
        "\n",
        "    def play_game(self):\n",
        "        for episode in range(self.episodes_times):\n",
        "\n",
        "            if (episode % 10 == 0):\n",
        "                metrics_names = ['score']\n",
        "                if (int(str(self.episodes_times)[:-1])*10 == episode):\n",
        "                    pb_i = Progbar(int(str(self.episodes_times)[-1]), stateful_metrics=metrics_names)\n",
        "                else:\n",
        "                    pb_i = Progbar(10, stateful_metrics=metrics_names)\n",
        "                score_mean = np.array([])\n",
        "\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "            self.experiences.reset_experiences()\n",
        "    \n",
        "            while not done:\n",
        "                self.env.render()              \n",
        "                action = self.actor.policynetwork(state)\n",
        "                tmp_action = self._action_clipping(action)\n",
        "                next_state, reward, done, info = self.env.step(tmp_action)\n",
        "                score+=reward\n",
        "\n",
        "                self.experiences.set_experiences(state, next_state, action, reward, done)\n",
        "                if self.experiences.isGetter():\n",
        "                    m_batch = self.experiences.get_experiences()\n",
        "                    self.critic.valuenetwork(m_batch)\n",
        "                    self.experiences.reset_experiences()\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            score_mean = np.append(score_mean, score)\n",
        "            values = [('score',np.mean(score_mean))]\n",
        "            pb_i.add(1, values=values)\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "    def _action_clipping(self, val):\n",
        "        actions = np.array([[ 0, 0, 0],  # [0]: straight\n",
        "                            [ 0, 1, 0],  # [1]: acceleration\n",
        "                            [ 0, 0, 1],  # [2]: decelerate\n",
        "                            [ 1, 0, 0],  # [3]: Turn right\n",
        "                            [-1, 0, 0]]) # [4]: Turn left\n",
        "        return actions[val]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDiUeMsMA4-8",
        "outputId": "c977d3dc-4d1a-40b6-fdd6-7aebeb62fd58"
      },
      "source": [
        "episodes_times = 300\n",
        "batch_size = 32\n",
        "\n",
        "actor = Actor()\n",
        "critic = Critic()\n",
        "experiences = ExperiencesMemory(batch_size = batch_size)\n",
        "gym.logger.set_level(40)\n",
        "env = gym.make('CarRacing-v0')\n",
        "env.unwrapped.verbose = 0\n",
        "env = wrappers.Monitor(env, './', force=True, video_callable=(lambda ep: ep % 25 == 0))\n",
        "main = Main(env, actor, critic, experiences, episodes_times)\n",
        "main.play_game()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 96, 96, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 47, 47, 32)   1568        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 11, 11, 64)   32832       ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 3, 3, 64)     36928       ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 576)          0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          295424      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 5)            2565        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            513         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 369,830\n",
            "Trainable params: 369,830\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "10/10 [==============================] - 198s 17s/step - score: -54.8261\n",
            "10/10 [==============================] - 171s 17s/step - score: -55.2086\n",
            "10/10 [==============================] - 183s 18s/step - score: -55.7764\n",
            "10/10 [==============================] - 171s 17s/step - score: -55.2308\n",
            "10/10 [==============================] - 172s 17s/step - score: -55.0706\n",
            "10/10 [==============================] - 184s 17s/step - score: -56.7227\n",
            "10/10 [==============================] - 170s 17s/step - score: -56.3167\n",
            "10/10 [==============================] - 181s 18s/step - score: -54.8629\n",
            "10/10 [==============================] - 170s 17s/step - score: -57.3633\n",
            "10/10 [==============================] - 168s 17s/step - score: -53.5121\n",
            "10/10 [==============================] - 180s 17s/step - score: -57.7828\n",
            "10/10 [==============================] - 168s 17s/step - score: -53.5486\n",
            "10/10 [==============================] - 181s 18s/step - score: -54.0895\n",
            "10/10 [==============================] - 169s 17s/step - score: -56.7154\n",
            "10/10 [==============================] - 167s 17s/step - score: -54.6969\n",
            "10/10 [==============================] - 179s 17s/step - score: -55.1204\n",
            "10/10 [==============================] - 167s 17s/step - score: -54.0557\n",
            "10/10 [==============================] - 180s 18s/step - score: -56.8666\n",
            "10/10 [==============================] - 169s 17s/step - score: -55.1304\n",
            "10/10 [==============================] - 168s 17s/step - score: -56.7107\n",
            "10/10 [==============================] - 179s 17s/step - score: -55.5780\n",
            "10/10 [==============================] - 168s 17s/step - score: -55.3644\n",
            "10/10 [==============================] - 180s 18s/step - score: -54.9517\n",
            "10/10 [==============================] - 166s 17s/step - score: -57.0097\n",
            "10/10 [==============================] - 166s 17s/step - score: -53.8805\n",
            "10/10 [==============================] - 177s 17s/step - score: -56.4376\n",
            "10/10 [==============================] - 164s 16s/step - score: -55.5376\n",
            "10/10 [==============================] - 177s 18s/step - score: -58.2412\n",
            "10/10 [==============================] - 164s 16s/step - score: -56.2199\n",
            "10/10 [==============================] - 165s 17s/step - score: -55.3508\n"
          ]
        }
      ]
    }
  ]
}