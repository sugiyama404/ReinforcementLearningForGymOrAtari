{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2cForBreakout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO8yQnhhdo6TDswszj/SfGb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/ReinforcementLearningForGymOrAtari/blob/main/A2C/A2CForBreakout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8bCDqyTz9Wp"
      },
      "source": [
        "!pip uninstall gym -y  > /dev/null 2>&1 # gym 0.17.3 was broken 2021/11/08\n",
        "!pip install gym gym[atari,accept-rom-license] tensorflow-addons > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cck23Zda0D63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a93d0e-d679-47f2-900b-3ee5515fef52"
      },
      "source": [
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, ReLU, Input, Lambda, Conv2D, Flatten\n",
        "from tensorflow.keras.losses import Huber\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.optimizers import RectifiedAdam\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "import math\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "!apt update  > /dev/null 2>&1\n",
        "!apt install xvfb  > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay  > /dev/null 2>&1\n",
        "from pyvirtualdisplay import Display\n",
        "d = Display()\n",
        "d.start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for external in metadata.entry_points().get(self.group, []):\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7ff3b1e15e50>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bISt9ojzX9E0"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self):\n",
        "\n",
        "        obs_shape = (210, 160, 3)\n",
        "        nb_actions = 4\n",
        "        opt = RectifiedAdam(learning_rate=0.0001, epsilon=0.001)\n",
        "        input_ = inputs = Input(shape=obs_shape)\n",
        "        loss=Huber()\n",
        "        common = Conv2D(16, kernel_size=(8, 8), strides=(4, 4), activation='relu')(inputs)\n",
        "        common = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), activation='relu')(common)\n",
        "        common = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu')(common)\n",
        "        common = Flatten()(common)\n",
        "\n",
        "        common = Dense(512, activation='relu')(common)\n",
        "        actor_layer  = Dense(nb_actions, activation=\"softmax\")(common)\n",
        "        critic_layer = Dense(1, activation=\"linear\")(common)\n",
        "\n",
        "        model = Model(input_, [actor_layer, critic_layer])\n",
        "        model.compile(loss = loss, optimizer=opt)\n",
        "        model.summary()\n",
        "        Brain.model = model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "croR9sYRYInR"
      },
      "source": [
        "class Actor(Brain):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def policynetwork(self, state):\n",
        "        ps, _ = self.model(np.array([state]))\n",
        "        act_p = ps[0].numpy()\n",
        "        act_p = act_p if (not (np.isnan(act_p).any())) else self._nan_to_zero_softmax(act_p)[0]\n",
        "        return np.random.choice(4, p=act_p)\n",
        "\n",
        "    def _nan_to_zero_softmax(self, x):\n",
        "        x[np.isnan(x)] = 0\n",
        "        return self._softmax(x)\n",
        "\n",
        "    def _softmax(self, x):\n",
        "        if (x.ndim == 1):\n",
        "            x = x[None,:]\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hYtz2AkYL_P"
      },
      "source": [
        "class Critic(Brain):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.gamma = 0.99\n",
        "        self.beta  = 0.1\n",
        "\n",
        "    def valuenetwork(self, val):\n",
        "        states, next_states, actions = val['state'], val['next_state'], val['act']\n",
        "        rewards, dones = val['reward'], val['done']\n",
        "\n",
        "        onehot_actions = tf.one_hot(actions, 4)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            act_p, v = Brain.model(states, training=True)\n",
        "            _, next_v = Brain.model(next_states, training=True)\n",
        "\n",
        "            a_pi = tf.reduce_sum(onehot_actions * act_p, axis=1, keepdims=True)\n",
        "            a_pi = tf.clip_by_value(a_pi, 1e-10, 1.0)\n",
        "\n",
        "            q = rewards + (1 - dones) * self.gamma * next_v\n",
        "            advantage = q - v\n",
        "\n",
        "            value_losses = self._value_losses(advantage)\n",
        "            policy_losses = self._policy_losses(advantage, a_pi, v)\n",
        "            total_loss = value_losses + policy_losses\n",
        "            loss = tf.reduce_mean(total_loss)\n",
        "\n",
        "        gradients = tape.gradient(loss, Brain.model.trainable_variables)\n",
        "        Brain.model.optimizer.apply_gradients(zip(gradients, Brain.model.trainable_variables))\n",
        "\n",
        "    def _value_losses(self,advantage):\n",
        "        return (advantage)**2\n",
        "\n",
        "    def _policy_losses(self,advantage,a_pi,v):\n",
        "\n",
        "        a = tf.math.log(a_pi) * advantage\n",
        "        b = self._entropy(v)\n",
        "        policy_losses = - ( a + b )\n",
        "        return policy_losses\n",
        "\n",
        "    def _entropy(self, v):\n",
        "        sigma = tf.math.reduce_std(v)\n",
        "        sigma = tf.math.square(sigma)\n",
        "        entropy = self.beta*0.5*(tf.math.log(2 * math.pi * sigma) + 1)\n",
        "        return entropy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRB07mLKYeVO"
      },
      "source": [
        "@dataclass\n",
        "class ExperiencesMemory:\n",
        "    state : np.ndarray = np.empty((0, 210, 160, 3))\n",
        "    next_state : np.ndarray = np.empty((0, 210, 160, 3))\n",
        "    action : np.ndarray = np.array([],int)\n",
        "    reward : np.ndarray = np.array([])\n",
        "    done : np.ndarray = np.array([])\n",
        "    batch_size : int = 32\n",
        "\n",
        "    def reset_experiences(self):\n",
        "        self.state = np.empty((0, 210, 160, 3))\n",
        "        self.next_state = np.empty((0, 210, 160, 3))\n",
        "        self.action = np.array([],int)\n",
        "        self.reward = np.array([])\n",
        "        self.done = np.array([])\n",
        "\n",
        "    def set_experiences(self, state, next_state, action, reward, done):\n",
        "        state = np.reshape(state, [1, 210, 160, 3])\n",
        "        self.state = np.append(self.state, state, axis=0)\n",
        "        next_state = np.reshape(next_state, [1, 210, 160, 3])\n",
        "        self.next_state = np.append(self.next_state, next_state, axis=0)\n",
        "        self.action = np.append(self.action, np.array(action))\n",
        "        self.reward = np.append(self.reward, np.array(reward))\n",
        "        self.done = np.append(self.done, np.array(done))\n",
        "\n",
        "    def get_experiences(self):\n",
        "        mb_index = np.random.choice(len(self.action), self.batch_size, replace=False)\n",
        "        key = ['state','next_state','act','reward','done']\n",
        "        value = [self.state[mb_index], self.next_state[mb_index],\n",
        "                 self.action[mb_index], self.reward[mb_index], self.done[mb_index]]\n",
        "        dict1=dict(zip(key,value))\n",
        "        return dict1\n",
        "\n",
        "    def isGetter(self):\n",
        "        return True if (len(self.action) > self.batch_size) else False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLHjudMOYvFJ"
      },
      "source": [
        "class Main:\n",
        "    def __init__(self, env, actor, critic, experiences, episodes_times = 1000):\n",
        "        self.env = env\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.experiences = experiences\n",
        "        self.episodes_times = episodes_times\n",
        "\n",
        "    def play_game(self):\n",
        "        for episode in range(self.episodes_times):\n",
        "\n",
        "            if (episode % 10 == 0):\n",
        "                metrics_names = ['score']\n",
        "                if (int(str(self.episodes_times)[:-1])*10 == episode):\n",
        "                    pb_i = Progbar(int(str(self.episodes_times)[-1]), stateful_metrics=metrics_names)\n",
        "                else:\n",
        "                    pb_i = Progbar(10, stateful_metrics=metrics_names)\n",
        "                score_mean = np.array([])\n",
        "\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "            self.experiences.reset_experiences()\n",
        "    \n",
        "            while not done:\n",
        "                self.env.render()              \n",
        "                action = self.actor.policynetwork(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                score+=reward\n",
        "\n",
        "                self.experiences.set_experiences(state, next_state, action, reward, done)\n",
        "                if self.experiences.isGetter():\n",
        "                    m_batch = self.experiences.get_experiences()\n",
        "                    self.critic.valuenetwork(m_batch)\n",
        "                    self.experiences.reset_experiences()\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            score_mean = np.append(score_mean, score)\n",
        "            values = [('score',np.mean(score_mean))]\n",
        "            pb_i.add(1, values=values)\n",
        "\n",
        "        self.env.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etb_iBrX0Dhr",
        "outputId": "1171485a-56e4-4d89-f4aa-b143e75c5b59"
      },
      "source": [
        "episodes_times = 300\n",
        "batch_size = 32\n",
        "\n",
        "actor = Actor()\n",
        "critic = Critic()\n",
        "experiences = ExperiencesMemory(batch_size = batch_size)\n",
        "gym.logger.set_level(40)\n",
        "env = gym.make('Breakout-v0')\n",
        "env.unwrapped.verbose = 0\n",
        "env = wrappers.Monitor(env, './', force=True, video_callable=(lambda ep: ep % 25 == 0))\n",
        "main = Main(env, actor, critic, experiences, episodes_times)\n",
        "main.play_game()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 210, 160, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 51, 39, 16)   3088        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 24, 18, 32)   8224        ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 22, 16, 32)   9248        ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 11264)        0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          5767680     ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 4)            2052        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            513         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,790,805\n",
            "Trainable params: 5,790,805\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "10/10 [==============================] - 61s 4s/step - score: 0.9000\n",
            "10/10 [==============================] - 46s 5s/step - score: 1.4000\n",
            "10/10 [==============================] - 45s 5s/step - score: 1.3000\n",
            "10/10 [==============================] - 38s 4s/step - score: 0.5000\n",
            "10/10 [==============================] - 47s 5s/step - score: 1.5000\n",
            "10/10 [==============================] - 41s 4s/step - score: 0.9000\n",
            "10/10 [==============================] - 48s 5s/step - score: 1.7000\n",
            "10/10 [==============================] - 54s 5s/step - score: 2.3000\n",
            "10/10 [==============================] - 43s 4s/step - score: 1.0000\n",
            "10/10 [==============================] - 49s 5s/step - score: 1.7000\n",
            "10/10 [==============================] - 43s 4s/step - score: 1.1000\n",
            "10/10 [==============================] - 41s 4s/step - score: 0.9000\n",
            "10/10 [==============================] - 41s 4s/step - score: 0.9000\n",
            "10/10 [==============================] - 49s 5s/step - score: 1.8000\n",
            "10/10 [==============================] - 40s 4s/step - score: 1.0000\n",
            "10/10 [==============================] - 43s 4s/step - score: 1.2000\n",
            "10/10 [==============================] - 39s 4s/step - score: 0.9000\n",
            "10/10 [==============================] - 44s 4s/step - score: 1.3000\n",
            "10/10 [==============================] - 47s 5s/step - score: 2.0000\n",
            "10/10 [==============================] - 37s 4s/step - score: 0.9000\n",
            "10/10 [==============================] - 45s 5s/step - score: 1.4000\n",
            "10/10 [==============================] - 46s 5s/step - score: 1.4000\n",
            "10/10 [==============================] - 42s 4s/step - score: 1.2000\n",
            "10/10 [==============================] - 52s 5s/step - score: 2.3000\n",
            "10/10 [==============================] - 41s 4s/step - score: 1.2000\n",
            "10/10 [==============================] - 46s 5s/step - score: 1.7000\n",
            "10/10 [==============================] - 42s 4s/step - score: 1.4000\n",
            "10/10 [==============================] - 38s 4s/step - score: 0.9000\n",
            "10/10 [==============================] - 45s 5s/step - score: 1.6000\n",
            "10/10 [==============================] - 37s 3s/step - score: 0.8000\n"
          ]
        }
      ]
    }
  ]
}